{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the energy experiments \n",
    "\n",
    "df1 = pd.read_csv(\"opower_experiment_1.csv\")\n",
    "df2 = pd.read_csv(\"opower_experiment_2.csv\")\n",
    "df3 = pd.read_csv(\"opower_experiment_3.csv\")\n",
    "df4 = pd.read_csv(\"opower_experiment_4.csv\")\n",
    "df5 = pd.read_csv(\"opower_experiment_5.csv\")\n",
    "df6 = pd.read_csv(\"opower_experiment_6.csv\")\n",
    "df7 = pd.read_csv(\"opower_experiment_7.csv\")\n",
    "\n",
    "df1[\"cluster_id\"] = 1  \n",
    "df2[\"cluster_id\"] = 2\n",
    "df3[\"cluster_id\"] = 3  \n",
    "df4[\"cluster_id\"] = 4  \n",
    "df5[\"cluster_id\"] = 5  \n",
    "df6[\"cluster_id\"] = 6  \n",
    "df7[\"cluster_id\"] = 7  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = [df1, df2, df3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_dfs_silo = []\n",
    "true_ates = []\n",
    "\n",
    "for i in range(len(df_array)):\n",
    "    # Leave-one-out setup\n",
    "    leave_out_df = df_array[i]\n",
    "    stay_in_dfs = pd.concat([df_array[j] for j in range(len(df_array)) if j != i])\n",
    "\n",
    "    # Label S = 1 for experimental sample, S = 0 for target population\n",
    "    stay_in_dfs[\"S\"] = 1\n",
    "    leave_out_df[\"S\"] = 0\n",
    "\n",
    "    # Rename variables\n",
    "    stay_in_dfs = stay_in_dfs.rename(columns={\"treatment\": \"T\", \"daily_kwh\": \"Y\"})\n",
    "    leave_out_df = leave_out_df.rename(columns={\"treatment\": \"T\", \"daily_kwh\": \"Y\"})\n",
    "\n",
    "    # Concatenate trial and target\n",
    "    energy_dfs = pd.concat([stay_in_dfs, leave_out_df])\n",
    "\n",
    "    # Save combined dataset\n",
    "    energy_dfs_silo.append(energy_dfs)\n",
    "\n",
    "    # === Compute true ATE in left-out (target) sample ===\n",
    "    target = leave_out_df\n",
    "    treated = target[target[\"T\"] == 1][\"Y\"]\n",
    "    control = target[target[\"T\"] == 0][\"Y\"]\n",
    "    ate = treated.mean() - control.mean()\n",
    "    true_ates.append(ate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# === Estimators ===\n",
    "from methods.ipsw import estimate_ate_ipsw\n",
    "\n",
    "from methods.aipsw import estimate_ate_aipsw\n",
    "from methods.outcomeM import estimate_ate_outcome\n",
    "\n",
    "from methods.stratification import StratifiedGeneralizabilityEstimator \n",
    "\n",
    "from methods.calibration import estimate_ate_calibration\n",
    "from methods.acalibration import estimate_ate_calibration_augmented\n",
    "\n",
    "# === Utilities ===\n",
    "from methods.bootstrap_utils import run_ate_with_bootstrap, run_clustered_bootstrap\n",
    "\n",
    "# === Models ===\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier, XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipsw_logit_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    IPSW estimator with logistic regression for estimating P(S=1 | X).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like\n",
    "        Outcome variable.\n",
    "    T : array-like\n",
    "        Treatment assignment.\n",
    "    S : array-like\n",
    "        Trial sample indicator (1=trial, 0=target).\n",
    "    X : array-like or pd.DataFrame\n",
    "        Covariates used to predict S.\n",
    "    kwargs :\n",
    "        Additional arguments to pass to estimate_ate_ipsw().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : object\n",
    "        Object with `.ate` attribute (and possibly others).\n",
    "    \"\"\"\n",
    "    logit_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "    logit_model.fit(X, S)\n",
    "    ps_hat = logit_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    result = estimate_ate_ipsw(\n",
    "        y=Y,\n",
    "        a=T,\n",
    "        s=S,\n",
    "        ps=ps_hat,\n",
    "        weight_type=\"inverse_odds\",\n",
    "        stabilized=True,\n",
    "        clip=(0.01, 50),\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def ipsw_xgb_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    IPSW estimator with XGBoost for estimating P(S=1 | X).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like\n",
    "        Outcome variable.\n",
    "    T : array-like\n",
    "        Treatment assignment.\n",
    "    S : array-like\n",
    "        Trial sample indicator (1=trial, 0=target).\n",
    "    X : array-like or pd.DataFrame\n",
    "        Covariates used to predict S.\n",
    "    kwargs :\n",
    "        Additional arguments to pass to estimate_ate_ipsw().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : object\n",
    "        Object with `.ate` attribute (and possibly others).\n",
    "    \"\"\"\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    xgb_model.fit(X, S)\n",
    "    ps_hat = xgb_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    result = estimate_ate_ipsw(\n",
    "        y=Y,\n",
    "        a=T,\n",
    "        s=S,\n",
    "        ps=ps_hat,\n",
    "        weight_type=\"inverse_odds\",\n",
    "        stabilized=True,\n",
    "        clip=(0.01, 50),\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def stratified_logit_estimator(Y, T, S, X, n_strata=5, **kwargs):\n",
    "    \"\"\"\n",
    "    Stratified ATE estimator using logistic regression for sampling score model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like\n",
    "        Outcome variable.\n",
    "    T : array-like\n",
    "        Treatment indicator.\n",
    "    S : array-like\n",
    "        Sample indicator (1 = trial, 0 = target).\n",
    "    X : array-like (DataFrame or ndarray)\n",
    "        Covariates.\n",
    "    n_strata : int\n",
    "        Number of strata (default: 5).\n",
    "    **kwargs : optional\n",
    "        Additional keyword arguments passed to StratifiedGeneralizabilityEstimator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : object\n",
    "        An object with attributes:\n",
    "        - .ate\n",
    "        - .stratum_ates\n",
    "        - .stratum_weights\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "\n",
    "    est = StratifiedGeneralizabilityEstimator(\n",
    "        n_strata=n_strata,\n",
    "        target=\"nontrial\",\n",
    "        sampling_model=model,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    est.fit(X=X, S=S)\n",
    "    result = est.estimate_ate(Y=Y, T=T, S=S)\n",
    "    return result\n",
    "\n",
    "def aipsw_logit_xgb_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    AIPSW with:\n",
    "      - sampling model: Logistic regression (S ~ X)\n",
    "      - outcome model:  XGBoost regressor (Y ~ X, within trial)\n",
    "    \"\"\"\n",
    "    # Define models\n",
    "    logit_sampler = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "    xgb_outcome = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Call your AIPSW convenience function\n",
    "    res = estimate_ate_aipsw(\n",
    "        y=Y,\n",
    "        a=T,\n",
    "        s=S,\n",
    "        X=X,\n",
    "        sampling_model=logit_sampler,\n",
    "        outcome_model=xgb_outcome,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return res\n",
    "\n",
    "def aipsw_logit_linear_estimator(Y, T, S, X, **kwargs):\n",
    "    logit_sampler = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "    linear_outcome = LinearRegression()\n",
    "\n",
    "    result = estimate_ate_aipsw(\n",
    "        y=Y,\n",
    "        a=T,\n",
    "        s=S,\n",
    "        X=X,\n",
    "        sampling_model=logit_sampler,\n",
    "        outcome_model=linear_outcome,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def aipsw_logit_xgb_estimator(Y, T, S, X, **kwargs):\n",
    "    logit_sampler = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "    xgb_outcome = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    result = estimate_ate_aipsw(\n",
    "        y=Y,\n",
    "        a=T,\n",
    "        s=S,\n",
    "        X=X,\n",
    "        sampling_model=logit_sampler,\n",
    "        outcome_model=xgb_outcome,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def aipsw_xgb_linear_estimator(Y, T, S, X, **kwargs):\n",
    "    xgb_sampler = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    linear_outcome = LinearRegression()\n",
    "\n",
    "    result = estimate_ate_aipsw(\n",
    "        y=Y,\n",
    "        a=T,\n",
    "        s=S,\n",
    "        X=X,\n",
    "        sampling_model=xgb_sampler,\n",
    "        outcome_model=linear_outcome,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def aipsw_xgb_xgb_estimator(Y, T, S, X, **kwargs):\n",
    "    xgb_sampler = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    xgb_outcome = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    result = estimate_ate_aipsw(\n",
    "        y=Y,\n",
    "        a=T,\n",
    "        s=S,\n",
    "        X=X,\n",
    "        sampling_model=xgb_sampler,\n",
    "        outcome_model=xgb_outcome,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def outcome_linear_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    Estimates ATE using an outcome model approach with linear regression.\n",
    "\n",
    "    Only uses trial units (S=1) to fit models for μ₁(X) and μ₀(X),\n",
    "    then extrapolates to target (S=0).\n",
    "\n",
    "    Returns a result object with .ate\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "\n",
    "    result = estimate_ate_outcome(\n",
    "        Y=Y,\n",
    "        T=T,\n",
    "        S=S,\n",
    "        X=X,\n",
    "        outcome_model=model,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def outcome_xgb_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    Estimates ATE using an outcome model approach with XGBoost.\n",
    "\n",
    "    Only uses trial units (S=1) to fit models for μ₁(X) and μ₀(X),\n",
    "    then extrapolates to target (S=0).\n",
    "\n",
    "    Returns a result object with .ate\n",
    "    \"\"\"\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    result = estimate_ate_outcome(\n",
    "        Y=Y,\n",
    "        T=T,\n",
    "        S=S,\n",
    "        X=X,\n",
    "        outcome_model=model,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def calib_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    Calibration weighting estimator\n",
    "\n",
    "    Returns:\n",
    "        result with `.ate`\n",
    "    \"\"\"\n",
    "\n",
    "    result = estimate_ate_calibration(\n",
    "        Y=Y,\n",
    "        T=T,\n",
    "        S=S,\n",
    "        X=X,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def aug_calib_xgb_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    Augmented calibration estimator using:\n",
    "    - XGBoost regressor for outcome model\n",
    "\n",
    "    Returns:\n",
    "        result with `.ate`\n",
    "    \"\"\"\n",
    "\n",
    "    xgb_outcome = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    result = estimate_ate_calibration_augmented(\n",
    "        Y=Y,\n",
    "        T=T,\n",
    "        S=S,\n",
    "        X=X,\n",
    "        outcome_model=xgb_outcome,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def aug_calib_linear_estimator(Y, T, S, X, **kwargs):\n",
    "    \"\"\"\n",
    "    Augmented calibration estimator using:\n",
    "    - XGBoost regressor for outcome model\n",
    "\n",
    "    Returns:\n",
    "        result with `.ate`\n",
    "    \"\"\"\n",
    "\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    result = estimate_ate_calibration_augmented(\n",
    "        Y=Y,\n",
    "        T=T,\n",
    "        S=S,\n",
    "        X=X,\n",
    "        outcome_model=linear_model,\n",
    "        target=\"nontrial\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def get_latex_table(estimator_list, estimator_names, X_list, df, n_boot=400, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Run a list of estimators and return a LaTeX table of ATE, SE, and CI.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator_list : list of callables\n",
    "        List of estimator functions (each should accept Y, T, S, X).\n",
    "    estimator_names : list of str\n",
    "        List of estimator names (same order as estimator_list).\n",
    "    X_list : list of str\n",
    "        List of column names in df to use as covariates.\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns Y, T, S, and X covariates.\n",
    "    n_boot : int\n",
    "        Number of bootstrap repetitions.\n",
    "    alpha : float\n",
    "        Significance level for confidence intervals (default 0.05).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    latex_str : str\n",
    "        LaTeX-formatted table string.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for name, estimator in zip(estimator_names, estimator_list):\n",
    "        try:\n",
    "            ate, se, lo, hi = run_ate_with_bootstrap(\n",
    "                estimator=estimator,\n",
    "                Y=df[\"Y\"].values,\n",
    "                T=df[\"T\"].values,\n",
    "                S=df[\"S\"].values,\n",
    "                X=df[X_list],\n",
    "                n_boot=n_boot,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "            results.append({\n",
    "                \"Estimator\": name,\n",
    "                \"ATE\": round(ate, 3),\n",
    "                \"SE\": round(se, 3),\n",
    "                \"95% CI\": f\"[{lo:.3f}, {hi:.3f}]\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Estimator\": name,\n",
    "                \"ATE\": \"ERROR\",\n",
    "                \"SE\": \"ERROR\",\n",
    "                \"95% CI\": str(e)\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    latex_str = df_results.to_latex(index=False, caption=\"Estimated ATEs with Bootstrapped CIs\", label=\"tab:ate_bootstrap\")\n",
    "    return latex_str\n",
    "\n",
    "def get_latex_table_clustered(estimator_list, estimator_names, X_list, df, cluster_col, n_boot=400, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Run a list of estimators and return a LaTeX table of ATE, SE, and CI\n",
    "    using clustered bootstrap by experiment/group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator_list : list of callables\n",
    "        Each estimator function must accept (Y, T, S, X) and return an ATE.\n",
    "    estimator_names : list of str\n",
    "        Labels for each estimator.\n",
    "    X_list : list of str\n",
    "        Covariate columns to use from df.\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns: Y, T, S, X covariates, and cluster_col.\n",
    "    cluster_col : str\n",
    "        Name of column identifying cluster (e.g., experiment or group).\n",
    "    n_boot : int\n",
    "        Number of bootstrap samples.\n",
    "    alpha : float\n",
    "        Confidence level (e.g., 0.05 → 95% CI).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    latex_str : str\n",
    "        LaTeX-formatted table string.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cluster_ids = df[cluster_col].values\n",
    "    Y = df[\"Y\"].values\n",
    "    T = df[\"T\"].values\n",
    "    S = df[\"S\"].values\n",
    "    X = df[X_list]\n",
    "\n",
    "    for name, estimator in zip(estimator_names, estimator_list):\n",
    "        try:\n",
    "            ate, se, lo, hi = run_clustered_bootstrap(\n",
    "                estimator_fn=estimator,\n",
    "                Y=Y,\n",
    "                T=T,\n",
    "                S=S,\n",
    "                X=X,\n",
    "                cluster_ids=cluster_ids,\n",
    "                n_boot=n_boot,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "            results.append({\n",
    "                \"Estimator\": name,\n",
    "                \"ATE\": round(ate, 3),\n",
    "                \"SE\": round(se, 3),\n",
    "                \"95% CI\": f\"[{lo:.3f}, {hi:.3f}]\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Estimator\": name,\n",
    "                \"ATE\": \"ERROR\",\n",
    "                \"SE\": \"ERROR\",\n",
    "                \"95% CI\": str(e)\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    latex_str = df_results.to_latex(index=False, caption=\"Estimated ATEs (Clustered Bootstrap)\", label=\"tab:ate_clustered\")\n",
    "    return latex_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Estimated ATEs with Bootstrapped CIs}\n",
      "\\label{tab:ate_bootstrap}\n",
      "\\begin{tabular}{lrrl}\n",
      "\\toprule\n",
      "Estimator & ATE & SE & 95% CI \\\\\n",
      "\\midrule\n",
      "IPSW (Logit) & -0.996000 & 0.012000 & [-1.021, -0.976] \\\\\n",
      "IPSW (XGBoost) & -1.111000 & 0.020000 & [-1.124, -1.054] \\\\\n",
      "Stratified (Logit) & -1.007000 & 0.012000 & [-1.031, -0.987] \\\\\n",
      "AIPSW (Logit x Linear) & -0.964000 & 0.006000 & [-0.973, -0.953] \\\\\n",
      "AIPSW (Logit x XGBRegressor) & -0.964000 & 0.007000 & [-0.977, -0.951] \\\\\n",
      "AIPSW (XGBClassifer x XGBRegressor) & -0.964000 & 0.007000 & [-0.977, -0.952] \\\\\n",
      "AIPSW (XGBClassifer x Linear) & -0.963000 & 0.006000 & [-0.972, -0.951] \\\\\n",
      "G-Formulation (Linear) & -0.964000 & 0.006000 & [-0.973, -0.953] \\\\\n",
      "G-Formulation (XGBRegressor) & -0.964000 & 0.007000 & [-0.977, -0.951] \\\\\n",
      "Calibration Weighing & -0.996000 & 0.012000 & [-1.021, -0.976] \\\\\n",
      "Aug. Calibration Weighing (Linear) & -0.964000 & 0.006000 & [-0.973, -0.953] \\\\\n",
      "Aug. Calibration Weighing (XGBRegressor) & -0.963000 & 0.007000 & [-0.977, -0.951] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_vars = ['temp','alpha_i','is_high_usage', 'lambda_t']\n",
    "\n",
    "df = energy_dfs_silo[0]\n",
    "\n",
    "latex_output = get_latex_table(\n",
    "    estimator_list=[\n",
    "        ipsw_logit_estimator,\n",
    "        ipsw_xgb_estimator,\n",
    "        stratified_logit_estimator, \n",
    "        aipsw_logit_linear_estimator,\n",
    "        aipsw_logit_xgb_estimator,\n",
    "        aipsw_xgb_xgb_estimator,\n",
    "        aipsw_xgb_linear_estimator,\n",
    "        outcome_linear_estimator,\n",
    "        outcome_xgb_estimator,\n",
    "        calib_estimator,\n",
    "        aug_calib_linear_estimator,\n",
    "        aug_calib_xgb_estimator\n",
    "    ],\n",
    "    \n",
    "    estimator_names=[\n",
    "        \"IPSW (Logit)\",\n",
    "        \"IPSW (XGBoost)\",\n",
    "        \"Stratified (Logit)\",\n",
    "        \"AIPSW (Logit x Linear)\",\n",
    "        \"AIPSW (Logit x XGBRegressor)\",\n",
    "        \"AIPSW (XGBClassifer x XGBRegressor)\",\n",
    "        \"AIPSW (XGBClassifer x Linear)\",\n",
    "        \"G-Formulation (Linear)\",\n",
    "        \"G-Formulation (XGBRegressor)\", \n",
    "        \"Calibration Weighing\",\n",
    "        \"Aug. Calibration Weighing (Linear)\",\n",
    "        \"Aug. Calibration Weighing (XGBRegressor)\",\n",
    "    ],\n",
    "    X_list=X_vars,\n",
    "    df=df,\n",
    "    n_boot=50,\n",
    "    alpha=0.05,\n",
    ")\n",
    "\n",
    "print(latex_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Estimated ATEs (Clustered Bootstrap)}\n",
      "\\label{tab:ate_clustered}\n",
      "\\begin{tabular}{lrrl}\n",
      "\\toprule\n",
      "Estimator & ATE & SE & 95% CI \\\\\n",
      "\\midrule\n",
      "IPSW (Logit) & -0.996000 & 0.307000 & [-1.326, -0.591] \\\\\n",
      "IPSW (XGBoost) & -1.111000 & 0.302000 & [-1.374, -0.615] \\\\\n",
      "Stratified (Logit) & -1.007000 & 0.232000 & [-1.261, -0.703] \\\\\n",
      "AIPSW (Logit x Linear) & -0.964000 & 0.231000 & [-1.213, -0.660] \\\\\n",
      "AIPSW (Logit x XGBRegressor) & -0.964000 & 0.229000 & [-1.231, -0.683] \\\\\n",
      "AIPSW (XGBClassifer x XGBRegressor) & -0.964000 & 0.229000 & [-1.231, -0.682] \\\\\n",
      "AIPSW (XGBClassifer x Linear) & -0.963000 & 0.231000 & [-1.214, -0.660] \\\\\n",
      "G-Formulation (Linear) & -0.964000 & 0.231000 & [-1.213, -0.660] \\\\\n",
      "G-Formulation (XGBRegressor) & -0.964000 & 0.229000 & [-1.231, -0.683] \\\\\n",
      "Calibration Weighing & -0.996000 & 0.307000 & [-1.326, -0.591] \\\\\n",
      "Aug. Calibration Weighing (Linear) & -0.964000 & 0.231000 & [-1.213, -0.660] \\\\\n",
      "Aug. Calibration Weighing (XGBRegressor) & -0.963000 & 0.228000 & [-1.231, -0.684] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_output = get_latex_table_clustered(\n",
    "    estimator_list=[\n",
    "        ipsw_logit_estimator,\n",
    "        ipsw_xgb_estimator,\n",
    "        stratified_logit_estimator, \n",
    "        aipsw_logit_linear_estimator,\n",
    "        aipsw_logit_xgb_estimator,\n",
    "        aipsw_xgb_xgb_estimator,\n",
    "        aipsw_xgb_linear_estimator,\n",
    "        outcome_linear_estimator,\n",
    "        outcome_xgb_estimator,\n",
    "        calib_estimator,\n",
    "        aug_calib_linear_estimator,\n",
    "        aug_calib_xgb_estimator\n",
    "    ],\n",
    "    \n",
    "    estimator_names=[\n",
    "        \"IPSW (Logit)\",\n",
    "        \"IPSW (XGBoost)\",\n",
    "        \"Stratified (Logit)\",\n",
    "        \"AIPSW (Logit x Linear)\",\n",
    "        \"AIPSW (Logit x XGBRegressor)\",\n",
    "        \"AIPSW (XGBClassifer x XGBRegressor)\",\n",
    "        \"AIPSW (XGBClassifer x Linear)\",\n",
    "        \"G-Formulation (Linear)\",\n",
    "        \"G-Formulation (XGBRegressor)\", \n",
    "        \"Calibration Weighing\",\n",
    "        \"Aug. Calibration Weighing (Linear)\",\n",
    "        \"Aug. Calibration Weighing (XGBRegressor)\",\n",
    "    ],\n",
    "    X_list=X_vars,\n",
    "    cluster_col= 'cluster_id',\n",
    "    df=df,\n",
    "    n_boot=100,\n",
    "    alpha=0.05,\n",
    ")\n",
    "\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7326964420450324\n"
     ]
    }
   ],
   "source": [
    "print(true_ates[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
